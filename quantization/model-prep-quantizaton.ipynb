{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":402407,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":329147,"modelId":349968}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Preparing a Custom Model for Quantization","metadata":{"id":"qfre7wbZHrMT"}},{"cell_type":"code","source":"!git clone https://github.com/aashu-0/FineTuning_GPT2.git\n%cd FineTuning_GPT2","metadata":{"id":"zlPC3B0NHqfj","outputId":"9bb048b2-6e4f-41f0-a56c-83a1f9e1922a","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:11:37.650995Z","iopub.execute_input":"2025-05-22T17:11:37.651172Z","iopub.status.idle":"2025-05-22T17:11:38.530666Z","shell.execute_reply.started":"2025-05-22T17:11:37.651155Z","shell.execute_reply":"2025-05-22T17:11:38.529814Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'FineTuning_GPT2'...\nremote: Enumerating objects: 277, done.\u001b[K\nremote: Counting objects: 100% (13/13), done.\u001b[K\nremote: Compressing objects: 100% (9/9), done.\u001b[K\nremote: Total 277 (delta 4), reused 13 (delta 4), pack-reused 264 (from 1)\u001b[K\nReceiving objects: 100% (277/277), 1.02 MiB | 8.73 MiB/s, done.\nResolving deltas: 100% (153/153), done.\n/kaggle/working/FineTuning_GPT2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# cwd\nimport os\nprint(os.getcwd())","metadata":{"id":"OO1kSbyEHxH8","outputId":"00b89687-71ac-4146-f1b5-d267cdfc5d56","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:11:38.531749Z","iopub.execute_input":"2025-05-22T17:11:38.532050Z","iopub.status.idle":"2025-05-22T17:11:38.536808Z","shell.execute_reply.started":"2025-05-22T17:11:38.532016Z","shell.execute_reply":"2025-05-22T17:11:38.536055Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/FineTuning_GPT2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip -q install tiktoken","metadata":{"id":"6JxNAiKqH1H_","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:11:38.538451Z","iopub.execute_input":"2025-05-22T17:11:38.539136Z","iopub.status.idle":"2025-05-22T17:11:42.868624Z","shell.execute_reply.started":"2025-05-22T17:11:38.539108Z","shell.execute_reply":"2025-05-22T17:11:42.867883Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from base_model.config import GPT2Config\nconfig = GPT2Config()","metadata":{"id":"KLH2jMPaH1mL","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:11:42.869715Z","iopub.execute_input":"2025-05-22T17:11:42.870048Z","iopub.status.idle":"2025-05-22T17:11:42.876612Z","shell.execute_reply.started":"2025-05-22T17:11:42.870015Z","shell.execute_reply":"2025-05-22T17:11:42.875787Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.quantization import QuantStub, DeQuantStub\nimport torch.nn.functional as F\nimport math\nfrom quantization.gpt2_quantization_prep import prepare_model_for_quantization","metadata":{"id":"jT0J9WPGIRPL","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:11:42.877544Z","iopub.execute_input":"2025-05-22T17:11:42.877870Z","iopub.status.idle":"2025-05-22T17:11:47.707374Z","shell.execute_reply.started":"2025-05-22T17:11:42.877829Z","shell.execute_reply":"2025-05-22T17:11:47.706873Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"gpt2 = prepare_model_for_quantization(\n    config = config,\n    lora_rank = 16,\n    lora_alpha = 16,\n    model_path = \"/kaggle/input/gpt2/pytorch/lora-finetuned/1/gpt2_lorafinetuned.pt\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:14:17.331150Z","iopub.execute_input":"2025-05-22T17:14:17.331424Z","iopub.status.idle":"2025-05-22T17:14:22.994952Z","shell.execute_reply.started":"2025-05-22T17:14:17.331404Z","shell.execute_reply":"2025-05-22T17:14:22.994260Z"}},"outputs":[{"name":"stdout","text":"Initializing GPT2 model with quantization stubs...\nInitial total parameters: 163,037,184\nInitial trainable parameters: 163,037,184\n\nFreezing base model parameters...\nTrainable parameters after freezing: 0\n\nInjecting LoRA layers (rank=16, alpha=16)...\nTrainable LoRA parameters: 3,175,696\n\nLoading pre-trained weights from: /kaggle/input/gpt2/pytorch/lora-finetuned/1/gpt2_lorafinetuned.pt\nSuccessfully loaded pre-trained weights!\n\nModel preparation complete!\nThe model is now ready for post-training quantization.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"gpt2","metadata":{"id":"t6g-Pa8FSGn3","outputId":"d7ecc473-84a2-4ad2-b8a0-00c2b8b1f8cf","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:14:31.980087Z","iopub.execute_input":"2025-05-22T17:14:31.980378Z","iopub.status.idle":"2025-05-22T17:14:31.987399Z","shell.execute_reply.started":"2025-05-22T17:14:31.980356Z","shell.execute_reply":"2025-05-22T17:14:31.986622Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"GPTModelQuantized(\n  (tok_emb): Embedding(50257, 768)\n  (pos_emb): Embedding(1024, 768)\n  (emb_dropout): Dropout(p=0.1, inplace=False)\n  (main_quant): QuantStub()\n  (main_dequant): DeQuantStub()\n  (trf_blocks): ModuleList(\n    (0-11): 12 x TransformerBlock(\n      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(in_features=768, out_features=2304, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=768, out_features=768, bias=True)\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub()\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub()\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): QuantStub()\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(in_features=768, out_features=3072, bias=True)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(in_features=3072, out_features=768, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub()\n      (residual_dequant): DeQuantStub()\n    )\n  )\n  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (out_head): LinearWithLoRA(\n    (linear): Linear(in_features=768, out_features=50257, bias=False)\n    (lora): LoRALayer()\n  )\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"gpt2.eval()","metadata":{"id":"PTI66KDkWnRD","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:14:45.913669Z","iopub.execute_input":"2025-05-22T17:14:45.914414Z","iopub.status.idle":"2025-05-22T17:14:45.921037Z","shell.execute_reply.started":"2025-05-22T17:14:45.914387Z","shell.execute_reply":"2025-05-22T17:14:45.920401Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"GPTModelQuantized(\n  (tok_emb): Embedding(50257, 768)\n  (pos_emb): Embedding(1024, 768)\n  (emb_dropout): Dropout(p=0.1, inplace=False)\n  (main_quant): QuantStub()\n  (main_dequant): DeQuantStub()\n  (trf_blocks): ModuleList(\n    (0-11): 12 x TransformerBlock(\n      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(in_features=768, out_features=2304, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=768, out_features=768, bias=True)\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub()\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub()\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): QuantStub()\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(in_features=768, out_features=3072, bias=True)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(in_features=3072, out_features=768, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub()\n      (residual_dequant): DeQuantStub()\n    )\n  )\n  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (out_head): LinearWithLoRA(\n    (linear): Linear(in_features=768, out_features=50257, bias=False)\n    (lora): LoRALayer()\n  )\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"print('Before Quantization')\nprint(gpt2.trf_blocks[0].attn.qkv.linear.weight)\nprint(gpt2.trf_blocks[0].attn.qkv.linear.weight.dtype)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:14:49.129638Z","iopub.execute_input":"2025-05-22T17:14:49.130223Z","iopub.status.idle":"2025-05-22T17:14:49.197600Z","shell.execute_reply.started":"2025-05-22T17:14:49.130197Z","shell.execute_reply":"2025-05-22T17:14:49.196800Z"}},"outputs":[{"name":"stdout","text":"Before Quantization\nParameter containing:\ntensor([[-0.4738,  0.0874,  0.0039,  ..., -0.2592,  0.1517, -0.4100],\n        [-0.2614,  0.1473,  0.0695,  ..., -0.0164,  0.2170, -0.1924],\n        [-0.0978,  0.2387,  0.3668,  ...,  0.1991,  0.1043, -0.2400],\n        ...,\n        [ 0.0513, -0.0525,  0.1143,  ...,  0.0095,  0.0293, -0.0046],\n        [-0.0584, -0.0113,  0.0363,  ..., -0.0516, -0.0429,  0.0070],\n        [ 0.0250, -0.0156, -0.0318,  ...,  0.0319, -0.0475,  0.0198]])\ntorch.float32\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"model_path = \"/kaggle/input/gpt2/pytorch/lora-finetuned/1/gpt2_lorafinetuned.pt\"\nimport os\ndef get_size(path):\n    return os.path.getsize(path) / 1e6  # MB\n    \nprint(f\"Model size before qunatization: {get_size(model_path):.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:15:22.611922Z","iopub.execute_input":"2025-05-22T17:15:22.612194Z","iopub.status.idle":"2025-05-22T17:15:22.625016Z","shell.execute_reply.started":"2025-05-22T17:15:22.612175Z","shell.execute_reply":"2025-05-22T17:15:22.624270Z"}},"outputs":[{"name":"stdout","text":"Model size before qunatization: 664.95 MB\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Post Training Quantization\nWorkFLow\n1. original model -> model (in `float32`)\n2. calibration:\n   - run a few batches of data through the model to capture **activation statistics** (min/max) which are then used for quantization of model\n   - these statistics are used to calculate quantized weights and activations\n3. Quantization mapping:\n   - for each layer/tensor, conversion from float to int using scale and zero-point.\n\n**Types of PTQ**\n1. Static\n2. Dynamic","metadata":{}},{"cell_type":"markdown","source":"### How to do quantize?\n1. insert min-max observers in the model using *quantization-aware* modules like `QuantStub` and `DeQuantStub`\n2. copy weights from the unquantized model\n3. specify the quantization configuration\n4. prepre the model for calibration\n5. convert to quantized version","metadata":{}},{"cell_type":"markdown","source":"From pytorch docs: https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\n1. set quantization config for server (x86) deployment\n`myModel.qconfig = torch.quantization.get_default_config('fbgemm')`\n2. insert observers\n`torch.quantization.prepare(myModel, inplace=True)`\n3. Calibrate the model and collect statistics\n`torch.quantization.convert(myModel, inplace=True) #convert to quantized version`","metadata":{}},{"cell_type":"code","source":"from torch.ao.quantization.qconfig import float_qparams_weight_only_qconfig\nfrom torch.ao.quantization import get_default_qconfig\n# qunatization configuration\ngpt2.qconfig = torch.ao.quantization.default_qconfig\n\n# Override specifically for embedding layers\nfor name, module in gpt2.named_modules():\n    if isinstance(module, torch.nn.Embedding):\n        module.qconfig = float_qparams_weight_only_qconfig\n\n# insert observers\ngpt2 = torch.quantization.prepare(gpt2)\ngpt2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:15:28.336355Z","iopub.execute_input":"2025-05-22T17:15:28.337035Z","iopub.status.idle":"2025-05-22T17:15:28.707196Z","shell.execute_reply.started":"2025-05-22T17:15:28.337010Z","shell.execute_reply":"2025-05-22T17:15:28.706475Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"GPTModelQuantized(\n  (tok_emb): Embedding(\n    50257, 768\n    (activation_post_process): PlaceholderObserver(dtype=torch.float32, is_dynamic=False)\n  )\n  (pos_emb): Embedding(\n    1024, 768\n    (activation_post_process): PlaceholderObserver(dtype=torch.float32, is_dynamic=False)\n  )\n  (emb_dropout): Dropout(p=0.1, inplace=False)\n  (main_quant): QuantStub(\n    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (main_dequant): DeQuantStub()\n  (trf_blocks): ModuleList(\n    (0-11): 12 x TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n  )\n  (final_norm): LayerNorm(\n    (768,), eps=1e-05, elementwise_affine=True\n    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (out_head): LinearWithLoRA(\n    (linear): Linear(\n      in_features=768, out_features=50257, bias=False\n      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n    )\n    (lora): LoRALayer()\n  )\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# calibarate using the test set\n# get the test data\nimport tiktoken\nfrom fine_tune.config import TrainingConfig\nfrom fine_tune.dataset import download_dataset,load_subset,train_test_split, create_dataloader\n\nconfig = TrainingConfig()\nfull_dataset = download_dataset(config)\nsub_dataset = load_subset(full_dataset, config)\n# split\ntrain_data, test_data, val_data = train_test_split(sub_dataset,config)\n\ntokenizer = tiktoken.get_encoding('gpt2')\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n#dataloaders\ntrain_loader, test_loader, val_loader = create_dataloader(\n    train_data, test_data, val_data, tokenizer, config, device=device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:15:47.702455Z","iopub.execute_input":"2025-05-22T17:15:47.703181Z","iopub.status.idle":"2025-05-22T17:15:50.326655Z","shell.execute_reply.started":"2025-05-22T17:15:47.703157Z","shell.execute_reply":"2025-05-22T17:15:50.325899Z"}},"outputs":[{"name":"stdout","text":"Data Loaded Successfully\nNumber of entries in dataset: 51760\nSubset Data loaded successfully\nNumber of entries in subset dataset: 3000\nTrain set size: 2550\nTest set size: 150\nValidation set size: 300\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"for idx, (X, y) in enumerate(test_loader):\n    print(f'Input: {X} \\n Target: {y}')\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:15:51.890286Z","iopub.execute_input":"2025-05-22T17:15:51.890564Z","iopub.status.idle":"2025-05-22T17:15:52.121258Z","shell.execute_reply.started":"2025-05-22T17:15:51.890542Z","shell.execute_reply":"2025-05-22T17:15:52.120592Z"}},"outputs":[{"name":"stdout","text":"Input: tensor([[21106,   318,   281,  ...,  2095,  7138,    13],\n        [21106,   318,   281,  ..., 50256, 50256, 50256],\n        [21106,   318,   281,  ..., 50256, 50256, 50256],\n        ...,\n        [21106,   318,   281,  ..., 50256, 50256, 50256],\n        [21106,   318,   281,  ..., 50256, 50256, 50256],\n        [21106,   318,   281,  ..., 50256, 50256, 50256]], device='cuda:0') \n Target: tensor([[  318,   281, 12064,  ...,  7138,    13, 50256],\n        [  318,   281, 12064,  ...,  -100,  -100,  -100],\n        [  318,   281, 12064,  ...,  -100,  -100,  -100],\n        ...,\n        [  318,   281, 12064,  ...,  -100,  -100,  -100],\n        [  318,   281, 12064,  ...,  -100,  -100,  -100],\n        [  318,   281, 12064,  ...,  -100,  -100,  -100]], device='cuda:0')\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef calibrate_model(model, dataloader, n_batches= 50):\n    model.eval()\n    with torch.no_grad():\n        for i, (batch_input, batch_label) in enumerate(tqdm(dataloader, desc= \"Calibrating\")):\n            if i >= n_batches:\n                break\n            input_ids = batch_input\n            input_ids = input_ids.to(next(model.parameters()).device)\n            _ = model(input_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:16:00.784380Z","iopub.execute_input":"2025-05-22T17:16:00.784650Z","iopub.status.idle":"2025-05-22T17:16:00.790041Z","shell.execute_reply.started":"2025-05-22T17:16:00.784631Z","shell.execute_reply":"2025-05-22T17:16:00.789255Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"calibrate_model(gpt2, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:16:03.478298Z","iopub.execute_input":"2025-05-22T17:16:03.478557Z","iopub.status.idle":"2025-05-22T17:18:08.069822Z","shell.execute_reply.started":"2025-05-22T17:16:03.478538Z","shell.execute_reply":"2025-05-22T17:18:08.069223Z"}},"outputs":[{"name":"stderr","text":"Calibrating: 100%|██████████| 19/19 [02:04<00:00,  6.56s/it]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"print(f'Check statistics of the various layers')\ngpt2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:18:13.428861Z","iopub.execute_input":"2025-05-22T17:18:13.429472Z","iopub.status.idle":"2025-05-22T17:18:13.441031Z","shell.execute_reply.started":"2025-05-22T17:18:13.429448Z","shell.execute_reply":"2025-05-22T17:18:13.440307Z"}},"outputs":[{"name":"stdout","text":"Check statistics of the various layers\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"GPTModelQuantized(\n  (tok_emb): Embedding(\n    50257, 768\n    (activation_post_process): PlaceholderObserver(dtype=torch.float32, is_dynamic=False)\n  )\n  (pos_emb): Embedding(\n    1024, 768\n    (activation_post_process): PlaceholderObserver(dtype=torch.float32, is_dynamic=False)\n  )\n  (emb_dropout): Dropout(p=0.1, inplace=False)\n  (main_quant): QuantStub(\n    (activation_post_process): MinMaxObserver(min_val=-4.535839557647705, max_val=3.9612929821014404)\n  )\n  (main_dequant): DeQuantStub()\n  (trf_blocks): ModuleList(\n    (0): TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-0.7718712091445923, max_val=0.8626592755317688)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-10.400775909423828, max_val=11.137125015258789)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-17.085010528564453, max_val=17.823265075683594)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-0.7718712091445923, max_val=0.8626592755317688)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-2.5222511291503906, max_val=1.8662253618240356)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-2.568769931793213, max_val=1.9856646060943604)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-2.568769931793213, max_val=1.9856646060943604)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-11.580428123474121, max_val=12.33021354675293)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-86.82018280029297, max_val=129.80946350097656)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=-0.7718712091445923, max_val=0.8626592755317688)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n    (1): TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-2.9926047325134277, max_val=2.6207518577575684)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-10.436954498291016, max_val=10.173405647277832)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-15.277487754821777, max_val=32.66456985473633)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-2.9926047325134277, max_val=2.6207518577575684)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-3.4454915523529053, max_val=3.446739912033081)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-3.1542809009552, max_val=6.3916497230529785)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-3.1542809009552, max_val=6.3916497230529785)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-15.878889083862305, max_val=15.501405715942383)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-11.560880661010742, max_val=408.794189453125)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=-2.9926047325134277, max_val=2.6207518577575684)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n    (2): TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-6.421692848205566, max_val=6.608725547790527)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-13.410818099975586, max_val=13.672971725463867)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-13.036920547485352, max_val=13.380914688110352)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-6.421692848205566, max_val=6.608725547790527)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-2.60215163230896, max_val=2.391268730163574)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-1.851662278175354, max_val=19.384414672851562)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-1.851662278175354, max_val=19.384414672851562)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-33.523494720458984, max_val=61.300865173339844)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-11.306047439575195, max_val=2206.780517578125)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=-6.421692848205566, max_val=6.608725547790527)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n    (3): TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-8.856103897094727, max_val=8.477035522460938)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-19.114303588867188, max_val=19.511165618896484)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-8.984790802001953, max_val=18.199739456176758)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-8.856103897094727, max_val=8.477035522460938)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-2.863316297531128, max_val=3.088099956512451)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-1.9671921730041504, max_val=8.585741996765137)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-1.9671921730041504, max_val=8.585741996765137)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-14.590130805969238, max_val=10.334005355834961)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-14.052967071533203, max_val=145.73318481445312)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=-8.856103897094727, max_val=8.477035522460938)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n    (4): TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-8.809053421020508, max_val=7.159803867340088)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-28.311460494995117, max_val=27.975330352783203)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-8.910036087036133, max_val=15.266851425170898)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-8.809053421020508, max_val=7.159803867340088)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-3.7572531700134277, max_val=3.30438494682312)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-2.703360080718994, max_val=8.444439888000488)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-2.703360080718994, max_val=8.444439888000488)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-8.754448890686035, max_val=6.537282943725586)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-19.29242515563965, max_val=153.04525756835938)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=-8.809053421020508, max_val=7.159803867340088)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n    (5): TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-10.16555118560791, max_val=7.102771282196045)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-20.024314880371094, max_val=14.812834739685059)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-13.783989906311035, max_val=22.371959686279297)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-10.16555118560791, max_val=7.102771282196045)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-4.275880336761475, max_val=5.28788948059082)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-2.35117244720459, max_val=10.562700271606445)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-2.35117244720459, max_val=10.562700271606445)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-9.983329772949219, max_val=7.128427505493164)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-13.348894119262695, max_val=85.24612426757812)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=-10.16555118560791, max_val=7.102771282196045)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n    (6): TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-9.463911056518555, max_val=6.982998371124268)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-14.78983211517334, max_val=12.891763687133789)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-12.44275188446045, max_val=27.50843048095703)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-9.463911056518555, max_val=6.982998371124268)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-4.7359418869018555, max_val=3.6096842288970947)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-2.2946319580078125, max_val=9.833728790283203)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-2.2946319580078125, max_val=9.833728790283203)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-9.930252075195312, max_val=7.0605692863464355)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-28.236865997314453, max_val=54.426883697509766)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=-9.463911056518555, max_val=6.982998371124268)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n    (7): TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-9.417411804199219, max_val=6.758426189422607)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-13.685382843017578, max_val=16.534759521484375)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-16.521961212158203, max_val=42.639915466308594)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-9.417411804199219, max_val=6.758426189422607)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-5.474176406860352, max_val=4.92093563079834)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-2.036191940307617, max_val=9.497538566589355)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-2.036191940307617, max_val=9.497538566589355)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-8.914735794067383, max_val=5.934148788452148)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-26.012434005737305, max_val=33.09153366088867)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=-9.417411804199219, max_val=6.758426189422607)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n    (8): TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-9.2073974609375, max_val=5.791820526123047)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-16.843645095825195, max_val=12.446745872497559)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-20.144184112548828, max_val=43.31475830078125)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-9.2073974609375, max_val=5.791820526123047)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-4.380648136138916, max_val=4.824923038482666)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-1.9845516681671143, max_val=7.712413311004639)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-1.9845516681671143, max_val=7.712413311004639)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-8.312187194824219, max_val=7.008352279663086)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-28.707666397094727, max_val=28.408361434936523)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=-9.2073974609375, max_val=5.791820526123047)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n    (9): TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-8.367386817932129, max_val=4.899957656860352)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-14.650534629821777, max_val=11.818572044372559)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-35.007545471191406, max_val=41.03001022338867)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-8.367386817932129, max_val=4.899957656860352)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-5.182232856750488, max_val=5.063041687011719)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-1.9426857233047485, max_val=6.552951335906982)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-1.9426857233047485, max_val=6.552951335906982)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-10.618391036987305, max_val=8.300910949707031)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-37.564788818359375, max_val=47.49504089355469)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=-8.367386817932129, max_val=4.899957656860352)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n    (10): TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-5.899343967437744, max_val=4.4171977043151855)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-12.737878799438477, max_val=10.487833976745605)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-64.25907135009766, max_val=62.7938346862793)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-5.899343967437744, max_val=4.4171977043151855)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-6.06810998916626, max_val=6.782130718231201)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-2.1343047618865967, max_val=7.66491174697876)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-2.1343047618865967, max_val=7.66491174697876)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-14.351557731628418, max_val=15.901191711425781)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-108.59554290771484, max_val=104.645751953125)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=-5.899343967437744, max_val=4.4171977043151855)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n    (11): TransformerBlock(\n      (ln1): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-5.072172164916992, max_val=5.079549789428711)\n      )\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=2304, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-13.083901405334473, max_val=12.984590530395508)\n          )\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-2512.124755859375, max_val=120.9052963256836)\n          )\n          (lora): LoRALayer()\n        )\n        (res_dropout): Dropout(p=0.1, inplace=False)\n        (input_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-5.072172164916992, max_val=5.079549789428711)\n        )\n        (input_dequant): DeQuantStub()\n        (output_quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-13.071772575378418, max_val=13.071208953857422)\n        )\n        (output_dequant): DeQuantStub()\n      )\n      (ln2): LayerNorm(\n        (768,), eps=1e-05, elementwise_affine=True\n        (activation_post_process): MinMaxObserver(min_val=-3.3705148696899414, max_val=6.376932144165039)\n      )\n      (mlp): MLP(\n        (quant): QuantStub(\n          (activation_post_process): MinMaxObserver(min_val=-3.3705148696899414, max_val=6.376932144165039)\n        )\n        (dequant): DeQuantStub()\n        (fc1): LinearWithLoRA(\n          (linear): Linear(\n            in_features=768, out_features=3072, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-26.058399200439453, max_val=28.834239959716797)\n          )\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): Linear(\n            in_features=3072, out_features=768, bias=True\n            (activation_post_process): MinMaxObserver(min_val=-193.95169067382812, max_val=229.70497131347656)\n          )\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (residual_quant): QuantStub(\n        (activation_post_process): MinMaxObserver(min_val=-5.072172164916992, max_val=5.079549789428711)\n      )\n      (residual_dequant): DeQuantStub()\n    )\n  )\n  (final_norm): LayerNorm(\n    (768,), eps=1e-05, elementwise_affine=True\n    (activation_post_process): MinMaxObserver(min_val=-203.19261169433594, max_val=208.2913360595703)\n  )\n  (out_head): LinearWithLoRA(\n    (linear): Linear(\n      in_features=768, out_features=50257, bias=False\n      (activation_post_process): MinMaxObserver(min_val=-276.6028137207031, max_val=146.4951171875)\n    )\n    (lora): LoRALayer()\n  )\n)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# quantize the model using the statistics collected\nquantized_gpt2 = torch.quantization.convert(gpt2, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:18:20.144238Z","iopub.execute_input":"2025-05-22T17:18:20.144494Z","iopub.status.idle":"2025-05-22T17:18:21.502307Z","shell.execute_reply.started":"2025-05-22T17:18:20.144474Z","shell.execute_reply":"2025-05-22T17:18:21.501763Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"print(f'Check statistics of the various layers')\nquantized_gpt2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:18:23.543620Z","iopub.execute_input":"2025-05-22T17:18:23.544213Z","iopub.status.idle":"2025-05-22T17:18:24.022912Z","shell.execute_reply.started":"2025-05-22T17:18:23.544188Z","shell.execute_reply":"2025-05-22T17:18:24.022212Z"}},"outputs":[{"name":"stdout","text":"Check statistics of the various layers\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"GPTModelQuantized(\n  (tok_emb): QuantizedEmbedding(num_embeddings=50257, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n  (pos_emb): QuantizedEmbedding(num_embeddings=1024, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n  (emb_dropout): QuantizedDropout(p=0.1, inplace=False)\n  (main_quant): Quantize(scale=tensor([0.0669]), zero_point=tensor([68]), dtype=torch.quint8)\n  (main_dequant): DeQuantize()\n  (trf_blocks): ModuleList(\n    (0): TransformerBlock(\n      (ln1): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.16958977282047272, zero_point=61, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=768, scale=0.27486830949783325, zero_point=62, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (res_dropout): QuantizedDropout(p=0.1, inplace=False)\n        (input_quant): Quantize(scale=tensor([0.0129]), zero_point=tensor([60]), dtype=torch.quint8)\n        (input_dequant): DeQuantize()\n        (output_quant): Quantize(scale=tensor([0.0346]), zero_point=tensor([73]), dtype=torch.quint8)\n        (output_dequant): DeQuantize()\n      )\n      (ln2): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): Quantize(scale=tensor([0.0359]), zero_point=tensor([72]), dtype=torch.quint8)\n        (dequant): DeQuantize()\n        (fc1): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=3072, scale=0.18827275931835175, zero_point=62, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=3072, out_features=768, scale=1.7057452201843262, zero_point=51, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (dropout): QuantizedDropout(p=0.1, inplace=False)\n      )\n      (residual_quant): Quantize(scale=tensor([0.0129]), zero_point=tensor([60]), dtype=torch.quint8)\n      (residual_dequant): DeQuantize()\n    )\n    (1): TransformerBlock(\n      (ln1): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.16228629648685455, zero_point=64, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=768, scale=0.37749651074409485, zero_point=40, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (res_dropout): QuantizedDropout(p=0.1, inplace=False)\n        (input_quant): Quantize(scale=tensor([0.0442]), zero_point=tensor([68]), dtype=torch.quint8)\n        (input_dequant): DeQuantize()\n        (output_quant): Quantize(scale=tensor([0.0543]), zero_point=tensor([63]), dtype=torch.quint8)\n        (output_dequant): DeQuantize()\n      )\n      (ln2): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): Quantize(scale=tensor([0.0752]), zero_point=tensor([42]), dtype=torch.quint8)\n        (dequant): DeQuantize()\n        (fc1): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=3072, scale=0.2470889389514923, zero_point=64, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=3072, out_features=768, scale=3.309882402420044, zero_point=3, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (dropout): QuantizedDropout(p=0.1, inplace=False)\n      )\n      (residual_quant): Quantize(scale=tensor([0.0442]), zero_point=tensor([68]), dtype=torch.quint8)\n      (residual_dequant): DeQuantize()\n    )\n    (2): TransformerBlock(\n      (ln1): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.21325819194316864, zero_point=63, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=768, scale=0.20801444351673126, zero_point=63, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (res_dropout): QuantizedDropout(p=0.1, inplace=False)\n        (input_quant): Quantize(scale=tensor([0.1026]), zero_point=tensor([63]), dtype=torch.quint8)\n        (input_dequant): DeQuantize()\n        (output_quant): Quantize(scale=tensor([0.0393]), zero_point=tensor([66]), dtype=torch.quint8)\n        (output_dequant): DeQuantize()\n      )\n      (ln2): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): Quantize(scale=tensor([0.1672]), zero_point=tensor([11]), dtype=torch.quint8)\n        (dequant): DeQuantize()\n        (fc1): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=3072, scale=0.7466484904289246, zero_point=45, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=3072, out_features=768, scale=17.46525001525879, zero_point=1, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (dropout): QuantizedDropout(p=0.1, inplace=False)\n      )\n      (residual_quant): Quantize(scale=tensor([0.1026]), zero_point=tensor([63]), dtype=torch.quint8)\n      (residual_dequant): DeQuantize()\n    )\n    (3): TransformerBlock(\n      (ln1): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.30413755774497986, zero_point=63, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=768, scale=0.21405142545700073, zero_point=42, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (res_dropout): QuantizedDropout(p=0.1, inplace=False)\n        (input_quant): Quantize(scale=tensor([0.1365]), zero_point=tensor([65]), dtype=torch.quint8)\n        (input_dequant): DeQuantize()\n        (output_quant): Quantize(scale=tensor([0.0469]), zero_point=tensor([61]), dtype=torch.quint8)\n        (output_dequant): DeQuantize()\n      )\n      (ln2): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): Quantize(scale=tensor([0.0831]), zero_point=tensor([24]), dtype=torch.quint8)\n        (dequant): DeQuantize()\n        (fc1): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=3072, scale=0.19625304639339447, zero_point=74, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=3072, out_features=768, scale=1.2581586837768555, zero_point=11, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (dropout): QuantizedDropout(p=0.1, inplace=False)\n      )\n      (residual_quant): Quantize(scale=tensor([0.1365]), zero_point=tensor([65]), dtype=torch.quint8)\n      (residual_dequant): DeQuantize()\n    )\n    (4): TransformerBlock(\n      (ln1): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.44320306181907654, zero_point=64, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=768, scale=0.19036918878555298, zero_point=47, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (res_dropout): QuantizedDropout(p=0.1, inplace=False)\n        (input_quant): Quantize(scale=tensor([0.1257]), zero_point=tensor([70]), dtype=torch.quint8)\n        (input_dequant): DeQuantize()\n        (output_quant): Quantize(scale=tensor([0.0556]), zero_point=tensor([68]), dtype=torch.quint8)\n        (output_dequant): DeQuantize()\n      )\n      (ln2): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): Quantize(scale=tensor([0.0878]), zero_point=tensor([31]), dtype=torch.quint8)\n        (dequant): DeQuantize()\n        (fc1): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=3072, scale=0.120407335460186, zero_point=73, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=3072, out_features=768, scale=1.3569896221160889, zero_point=14, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (dropout): QuantizedDropout(p=0.1, inplace=False)\n      )\n      (residual_quant): Quantize(scale=tensor([0.1257]), zero_point=tensor([70]), dtype=torch.quint8)\n      (residual_dequant): DeQuantize()\n    )\n    (5): TransformerBlock(\n      (ln1): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.2743082642555237, zero_point=73, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=768, scale=0.2846924960613251, zero_point=48, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (res_dropout): QuantizedDropout(p=0.1, inplace=False)\n        (input_quant): Quantize(scale=tensor([0.1360]), zero_point=tensor([75]), dtype=torch.quint8)\n        (input_dequant): DeQuantize()\n        (output_quant): Quantize(scale=tensor([0.0753]), zero_point=tensor([57]), dtype=torch.quint8)\n        (output_dequant): DeQuantize()\n      )\n      (ln2): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): Quantize(scale=tensor([0.1017]), zero_point=tensor([23]), dtype=torch.quint8)\n        (dequant): DeQuantize()\n        (fc1): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=3072, scale=0.1347382515668869, zero_point=74, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=3072, out_features=768, scale=0.7763386964797974, zero_point=17, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (dropout): QuantizedDropout(p=0.1, inplace=False)\n      )\n      (residual_quant): Quantize(scale=tensor([0.1360]), zero_point=tensor([75]), dtype=torch.quint8)\n      (residual_dequant): DeQuantize()\n    )\n    (6): TransformerBlock(\n      (ln1): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.21796531975269318, zero_point=68, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=768, scale=0.31457623839378357, zero_point=40, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (res_dropout): QuantizedDropout(p=0.1, inplace=False)\n        (input_quant): Quantize(scale=tensor([0.1295]), zero_point=tensor([73]), dtype=torch.quint8)\n        (input_dequant): DeQuantize()\n        (output_quant): Quantize(scale=tensor([0.0657]), zero_point=tensor([72]), dtype=torch.quint8)\n        (output_dequant): DeQuantize()\n      )\n      (ln2): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): Quantize(scale=tensor([0.0955]), zero_point=tensor([24]), dtype=torch.quint8)\n        (dequant): DeQuantize()\n        (fc1): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=3072, scale=0.13378599286079407, zero_point=74, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=3072, out_features=768, scale=0.6508956551551819, zero_point=43, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (dropout): QuantizedDropout(p=0.1, inplace=False)\n      )\n      (residual_quant): Quantize(scale=tensor([0.1295]), zero_point=tensor([73]), dtype=torch.quint8)\n      (residual_dequant): DeQuantize()\n    )\n    (7): TransformerBlock(\n      (ln1): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.23795387148857117, zero_point=58, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=768, scale=0.4658415615558624, zero_point=35, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (res_dropout): QuantizedDropout(p=0.1, inplace=False)\n        (input_quant): Quantize(scale=tensor([0.1274]), zero_point=tensor([74]), dtype=torch.quint8)\n        (input_dequant): DeQuantize()\n        (output_quant): Quantize(scale=tensor([0.0819]), zero_point=tensor([67]), dtype=torch.quint8)\n        (output_dequant): DeQuantize()\n      )\n      (ln2): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): Quantize(scale=tensor([0.0908]), zero_point=tensor([22]), dtype=torch.quint8)\n        (dequant): DeQuantize()\n        (fc1): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=3072, scale=0.1169203519821167, zero_point=76, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=3072, out_features=768, scale=0.4653855562210083, zero_point=56, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (dropout): QuantizedDropout(p=0.1, inplace=False)\n      )\n      (residual_quant): Quantize(scale=tensor([0.1274]), zero_point=tensor([74]), dtype=torch.quint8)\n      (residual_dequant): DeQuantize()\n    )\n    (8): TransformerBlock(\n      (ln1): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.2306329905986786, zero_point=73, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=768, scale=0.4996767044067383, zero_point=40, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (res_dropout): QuantizedDropout(p=0.1, inplace=False)\n        (input_quant): Quantize(scale=tensor([0.1181]), zero_point=tensor([78]), dtype=torch.quint8)\n        (input_dequant): DeQuantize()\n        (output_quant): Quantize(scale=tensor([0.0725]), zero_point=tensor([60]), dtype=torch.quint8)\n        (output_dequant): DeQuantize()\n      )\n      (ln2): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): Quantize(scale=tensor([0.0764]), zero_point=tensor([26]), dtype=torch.quint8)\n        (dequant): DeQuantize()\n        (fc1): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=3072, scale=0.12063416838645935, zero_point=69, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=3072, out_features=768, scale=0.4497325122356415, zero_point=64, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (dropout): QuantizedDropout(p=0.1, inplace=False)\n      )\n      (residual_quant): Quantize(scale=tensor([0.1181]), zero_point=tensor([78]), dtype=torch.quint8)\n      (residual_dequant): DeQuantize()\n    )\n    (9): TransformerBlock(\n      (ln1): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.20841816067695618, zero_point=70, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=768, scale=0.598720908164978, zero_point=58, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (res_dropout): QuantizedDropout(p=0.1, inplace=False)\n        (input_quant): Quantize(scale=tensor([0.1045]), zero_point=tensor([80]), dtype=torch.quint8)\n        (input_dequant): DeQuantize()\n        (output_quant): Quantize(scale=tensor([0.0807]), zero_point=tensor([64]), dtype=torch.quint8)\n        (output_dequant): DeQuantize()\n      )\n      (ln2): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): Quantize(scale=tensor([0.0669]), zero_point=tensor([29]), dtype=torch.quint8)\n        (dequant): DeQuantize()\n        (fc1): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=3072, scale=0.14897088706493378, zero_point=71, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=3072, out_features=768, scale=0.6697624325752258, zero_point=56, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (dropout): QuantizedDropout(p=0.1, inplace=False)\n      )\n      (residual_quant): Quantize(scale=tensor([0.1045]), zero_point=tensor([80]), dtype=torch.quint8)\n      (residual_dequant): DeQuantize()\n    )\n    (10): TransformerBlock(\n      (ln1): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.18287962675094604, zero_point=70, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=768, scale=1.0004165172576904, zero_point=64, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (res_dropout): QuantizedDropout(p=0.1, inplace=False)\n        (input_quant): Quantize(scale=tensor([0.0812]), zero_point=tensor([73]), dtype=torch.quint8)\n        (input_dequant): DeQuantize()\n        (output_quant): Quantize(scale=tensor([0.1012]), zero_point=tensor([60]), dtype=torch.quint8)\n        (output_dequant): DeQuantize()\n      )\n      (ln2): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): Quantize(scale=tensor([0.0772]), zero_point=tensor([28]), dtype=torch.quint8)\n        (dequant): DeQuantize()\n        (fc1): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=3072, scale=0.23821063339710236, zero_point=60, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=3072, out_features=768, scale=1.6790653467178345, zero_point=65, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (dropout): QuantizedDropout(p=0.1, inplace=False)\n      )\n      (residual_quant): Quantize(scale=tensor([0.0812]), zero_point=tensor([73]), dtype=torch.quint8)\n      (residual_dequant): DeQuantize()\n    )\n    (11): TransformerBlock(\n      (ln1): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): CausalMultiHeadAttention(\n        (qkv): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.20526371896266937, zero_point=64, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=768, scale=20.732519149780273, zero_point=121, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (res_dropout): QuantizedDropout(p=0.1, inplace=False)\n        (input_quant): Quantize(scale=tensor([0.0799]), zero_point=tensor([63]), dtype=torch.quint8)\n        (input_dequant): DeQuantize()\n        (output_quant): Quantize(scale=tensor([0.2059]), zero_point=tensor([64]), dtype=torch.quint8)\n        (output_dequant): DeQuantize()\n      )\n      (ln2): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): MLP(\n        (quant): Quantize(scale=tensor([0.0768]), zero_point=tensor([44]), dtype=torch.quint8)\n        (dequant): DeQuantize()\n        (fc1): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=768, out_features=3072, scale=0.4322254955768585, zero_point=60, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (gelu): GELU(approximate='tanh')\n        (fc2): LinearWithLoRA(\n          (linear): QuantizedLinear(in_features=3072, out_features=768, scale=3.335879325866699, zero_point=58, qscheme=torch.per_tensor_affine)\n          (lora): LoRALayer()\n        )\n        (dropout): QuantizedDropout(p=0.1, inplace=False)\n      )\n      (residual_quant): Quantize(scale=tensor([0.0799]), zero_point=tensor([63]), dtype=torch.quint8)\n      (residual_dequant): DeQuantize()\n    )\n  )\n  (final_norm): QuantizedLayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (out_head): LinearWithLoRA(\n    (linear): QuantizedLinear(in_features=768, out_features=50257, scale=3.331479787826538, zero_point=83, qscheme=torch.per_tensor_affine)\n    (lora): LoRALayer()\n  )\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"print('After Quantization')\nprint(torch.int_repr(quantized_gpt2.trf_blocks[0].attn.qkv.linear.weight()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:18:34.448296Z","iopub.execute_input":"2025-05-22T17:18:34.448790Z","iopub.status.idle":"2025-05-22T17:18:34.458989Z","shell.execute_reply.started":"2025-05-22T17:18:34.448767Z","shell.execute_reply":"2025-05-22T17:18:34.458300Z"}},"outputs":[{"name":"stdout","text":"After Quantization\ntensor([[-21,   4,   0,  ..., -12,   7, -18],\n        [-12,   7,   3,  ...,  -1,  10,  -9],\n        [ -4,  11,  16,  ...,   9,   5, -11],\n        ...,\n        [  2,  -2,   5,  ...,   0,   1,   0],\n        [ -3,  -1,   2,  ...,  -2,  -2,   0],\n        [  1,  -1,  -1,  ...,   1,  -2,   1]], dtype=torch.int8)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp_delme.p\")\n    print('Size (MB):', os.path.getsize(\"temp_delme.p\")/1e6)\n    os.remove('temp_delme.p')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:18:38.194154Z","iopub.execute_input":"2025-05-22T17:18:38.194414Z","iopub.status.idle":"2025-05-22T17:18:38.198497Z","shell.execute_reply.started":"2025-05-22T17:18:38.194393Z","shell.execute_reply":"2025-05-22T17:18:38.197860Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"print_size_of_model(quantized_gpt2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:18:41.120372Z","iopub.execute_input":"2025-05-22T17:18:41.120937Z","iopub.status.idle":"2025-05-22T17:18:41.848441Z","shell.execute_reply.started":"2025-05-22T17:18:41.120913Z","shell.execute_reply":"2025-05-22T17:18:41.847619Z"}},"outputs":[{"name":"stdout","text":"Size (MB): 176.690735\n","output_type":"stream"}],"execution_count":23}]}