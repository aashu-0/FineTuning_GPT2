{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"### 1. BERTScore\n- compares semantic similarity instead of raw words\n- great for capturing meaning, not just word\n- *how similar in meaning is our model's output to the actual output (reference), even if words differ*\n\n**Metrices**\n  - Precision(P)\n  - Recall(R)\n  - F1 score(F1): harmonic mean of P and R","metadata":{}},{"cell_type":"code","source":"import json\nimport urllib.request\nfrom bert_score import score\n\ndef load_data(url, file_name):\n    urllib.request.urlretrieve(url, file_name)\n    # load the JSON\n    with open(file_name, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n        print('Data Loaded Successfully')\n        print(f'Number of entries: {len(data)}')\n    return data\n\nfile_id = \"1s3hQ4d2soSFyVerNrJK9ihnGxmSga_96\"\nurl = f\"https://drive.google.com/uc?export=download&id={file_id}\"\nfile_name = \"eval.json\"\ndata = load_data(url, file_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T14:43:48.215619Z","iopub.execute_input":"2025-04-08T14:43:48.215858Z","iopub.status.idle":"2025-04-08T14:44:02.272403Z","shell.execute_reply.started":"2025-04-08T14:43:48.215838Z","shell.execute_reply":"2025-04-08T14:44:02.271640Z"}},"outputs":[{"name":"stdout","text":"Data Loaded Successfully\nNumber of entries: 2588\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# eval function\ndef evaluate_bertscore(data, threshold=0.7, show_bad=False):\n    references = [ex[\"output\"] for ex in data if ex.get(\"model_response\", \"\").strip()]\n                                #only keep this example if the model_response exists and is not blank/just whitespace.\n    predictions = [ex[\"model_response\"] for ex in data if ex.get(\"model_response\", \"\").strip()]\n    inputs = [ex for ex in data if ex.get(\"model_response\", \"\").strip()]\n\n    print(f\"Evaluatinng {len(predictions)} examples with BERTScore....\\n\")\n\n   # precision, recall, f1 \n    P, R, F1 = score(predictions, references, lang='en',model_type =\"bert-base-uncased\")\n\n    print(f\"Average BERTScore Precision: {P.mean().item():.4f}\")\n    print(f\"Average BERTScore Recall: {R.mean().item():.4f}\")\n    print(f\"Average BERTScore F1: {F1.mean().item():.4f}\")\n\n    # to print low-scoring examples.\n    if show_bad:\n        print(f'Evaluating examples with low f1 score(below the {threshold})...')\n        for i, (ex, ref, pred, f1_score) in enumerate(zip(inputs, references, predictions, F1)):\n            if f1_score< threshold:\n                print(f\"\\nExample {i+1}\")\n                print(f\"Instruction: {ex['instruction']}\")\n                print(f\"Input: {ex['input']}\")\n                print(f\"Expected: {ref}\")\n                print(f\"Model: {pred}\")\n                print(f\"F1 Score: {f1_score:.4f}\")\n                print(\"-\" * 50)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T14:44:02.273573Z","iopub.execute_input":"2025-04-08T14:44:02.273990Z","iopub.status.idle":"2025-04-08T14:44:02.281379Z","shell.execute_reply.started":"2025-04-08T14:44:02.273958Z","shell.execute_reply":"2025-04-08T14:44:02.280619Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"evaluate_bertscore(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T14:46:41.852251Z","iopub.execute_input":"2025-04-08T14:46:41.852559Z","iopub.status.idle":"2025-04-08T14:47:26.196181Z","shell.execute_reply.started":"2025-04-08T14:46:41.852533Z","shell.execute_reply":"2025-04-08T14:47:26.194983Z"}},"outputs":[{"name":"stdout","text":"Evaluatinng 2588 examples with BERTScore....\n\nAverage BERTScore Precision: 0.5568\nAverage BERTScore Recall: 0.5571\nAverage BERTScore F1: 0.5512\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"metrices are not quite good...can be improved further","metadata":{}},{"cell_type":"markdown","source":"## 2. BELU( Bilingual Evaluation Understudy)\n- precision based metric\n- checks how many n-grams in our model's response match the reference(actual Output)\n- doesb't care whether the model is adding any extra thing","metadata":{}},{"cell_type":"code","source":"#BELU-4\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport numpy as np\n\ndef compute_bleu(data, weights=(0.25, 0.25, 0.25, 0.25)):\n    smoothie = SmoothingFunction().method4\n    bleu_scores = []\n\n    for ex in data:\n        reference = ex['output'].strip().split()\n        hypothesis = ex['model_response'].strip().split()\n\n        score = sentence_bleu([reference], hypothesis, weights=weights, smoothing_function=smoothie)\n        bleu_scores.append(score)\n\n    return np.mean(bleu_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:06:23.421602Z","iopub.execute_input":"2025-04-08T15:06:23.421925Z","iopub.status.idle":"2025-04-08T15:06:23.427066Z","shell.execute_reply.started":"2025-04-08T15:06:23.421897Z","shell.execute_reply":"2025-04-08T15:06:23.426226Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"compute_bleu(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:07:45.610150Z","iopub.execute_input":"2025-04-08T15:07:45.610509Z","iopub.status.idle":"2025-04-08T15:07:46.451780Z","shell.execute_reply.started":"2025-04-08T15:07:45.610484Z","shell.execute_reply":"2025-04-08T15:07:46.451015Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0.09139332085559292"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"not good as expected!","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}